# 项目结构说明

## 一、项目概述

**项目名称**：说话人脸生成对话系统（TalkingGaussian UI System）

**项目功能**：这是一个基于AI技术的数字人对话系统，整合了多个先进的AI模型，能够实现：
1. **模型训练**：训练个性化的3D说话人头像模型
2. **视频生成**：根据音频生成对应的说话人头像视频
3. **实时对话**：完整的语音交互流程（语音识别 → 大语言模型 → 语音合成 → 视频生成）

## 二、技术栈

### 后端技术
- **Web框架**：Flask 3.0.3
- **AI模型集成**：
  - TalkingGaussian：基于Gaussian Splatting的3D说话人头像合成
  - CosyVoice：零样本语音克隆技术
  - SyncTalk：说话人头像生成（备选方案）
- **语音识别（ASR）**：Vosk（离线语音识别，支持中英文）
- **大语言模型（LLM）**：支持多平台API
  - Zhipu AI（智谱AI）
  - OpenAI
  - DeepSeek

### 前端技术
- HTML5 + CSS3 + JavaScript
- 响应式设计，支持主题切换和全屏模式

### 依赖环境
- **Docker环境**（推荐）：使用Docker容器化部署，包含所有依赖
- **本地环境**（可选）：
  - Python 3.x
  - CUDA（用于GPU加速）
  - Conda环境（talking_gaussian、cosyvoice、tg_eval、tg_niqe等）

## 三、项目目录结构

```
TFG_ui/
├── app.py                          # Flask主应用入口，定义所有路由
├── requirements.txt                # Python依赖包列表
├── README.md                       # 项目说明文档
├── Dockerfile                      # Docker镜像构建文件（主服务）
├── Dockerfile.eval                 # Docker镜像构建文件（评测服务）
├── docker-compose.yml              # Docker Compose配置文件
├── .dockerignore                   # Docker构建忽略文件
├── cleanup_docker_failed.sh        # Docker构建失败清理脚本
├── DOCKER完整指南.md               # Docker详细使用文档
│
├── backend/                        # 后端核心逻辑
│   ├── __init__.py
│   ├── video_generator.py          # 视频生成模块（调用TalkingGaussian/SyncTalk）
│   ├── model_trainer.py            # 模型训练模块
│   ├── chat_engine.py              # 对话系统引擎（ASR→LLM→TTS→Video）
│   ├── llm_service.py              # 大语言模型服务（统一API接口）
│   └── config/                     # 配置文件目录
│       ├── api_config.example.json # API配置示例文件
│       └── api_config.json         # API配置文件（需自行创建，已加入.gitignore）
│
├── templates/                      # HTML模板文件
│   ├── index.html                  # 首页（功能选择界面）
│   ├── model_training.html         # 模型训练页面
│   ├── video_generation.html       # 视频生成页面
│   └── chat_system.html            # 实时对话系统页面
│
├── static/                         # 静态资源文件（统一的对外文件入口）
│   ├── css/                        # 样式文件
│   ├── js/                         # JavaScript文件
│   ├── uploads/                    # 用户上传与系统整理后的可复用资源（方案B）
│   │   ├── videos/                 # 训练视频上传入口
│   │   │   └── May.mp4            # 示例：用于训练的参考视频
│   │   └── audios/                 # 参考音频（训练完成后自动整理）
│   │       └── may_reference.wav   # 示例：从 May.mp4 自动提取并复制出的参考音频
│   ├── audios/                     # 中间音频文件目录（实时对话与流程中间产物）
│   │   ├── input.wav               # 用户录音输入（主文件，会被覆盖）
│   │   ├── tts_output.wav          # 语音合成输出（主文件，会被覆盖）
│   │   └── custom_voice/           # 自定义语音克隆参考音频（带时间戳，不覆盖）
│   ├── videos/                     # 生成的视频文件（带时间戳，不覆盖）
│   └── text/                       # 文本文件（ASR识别结果、LLM回复等）
│       ├── input.txt               # ASR识别结果（主文件，会被覆盖）
│       └── output.txt              # LLM生成的回复（主文件，会被覆盖）
│
├── TalkingGaussian/                # TalkingGaussian模型相关代码
│   ├── README.md                   # TalkingGaussian使用说明
│   ├── .gitmodules                 # Git子模块配置（包含submodules）
│   ├── run_talkinggaussian.sh      # 推理脚本封装
│   ├── scripts/                    # 训练和工具脚本
│   │   ├── train_xx.sh           # 三阶段训练脚本
│   │   ├── prepare.sh             # 环境准备脚本
│   │   └── asr_transcribe.py      # ASR转录辅助工具
│   ├── evaluation/                 # 评测脚本目录
│   │   ├── run_all_metrics.sh     # 统一评测脚本（PSNR/SSIM/NIQE/FID/LSE）
│   │   ├── eval_frame_metrics.py  # 帧级指标（PSNR/SSIM/LPIPS）
│   │   ├── eval_niqe.py           # NIQE指标
│   │   ├── eval_fid.py             # FID指标
│   │   ├── eval_lse.py             # LSE-C/LSE-D指标
│   │   ├── extract_frames.py      # 视频抽帧工具
│   │   └── setup_lse.sh            # LSE评测环境设置
│   ├── data_utils/                 # 数据处理工具
│   │   ├── process.py              # 视频预处理脚本
│   │   ├── deepspeech_features/    # DeepSpeech特征提取
│   │   └── hubert.py               # HuBERT特征提取
│   ├── data/                       # 训练数据目录（由系统自动维护）
│   │   └── <ID>/                   # 每个ID一个子目录（例如 May/）
│   │       ├── <ID>.mp4            # 训练视频（从 static/uploads/videos/<ID>.mp4 自动复制）
│   │       ├── aud.wav             # 从训练视频中提取的音频
│   │       ├── transforms_train.json  # 预处理后的数据
│   │       └── au.csv              # Action Units数据
│   ├── output/                     # 训练好的模型输出目录（由系统自动生成）
│   │   └── <project_name>/         # 项目名称（通常与 <ID> 相同，如 May）
│   │       ├── chkpnt_face_latest.pth
│   │       ├── chkpnt_mouth_latest.pth
│   │       └── chkpnt_fuse_latest.pth
│   ├── test_result/                # 测试结果目录
│   ├── train_face.py               # 面部训练脚本
│   ├── train_mouth.py              # 嘴部训练脚本
│   ├── train_fuse.py               # 融合训练脚本
│   ├── synthesize_fuse.py          # 推理合成脚本
│   ├── submodules/                 # Git子模块（需初始化）
│   │   ├── simple-knn/             # simple-knn子模块
│   │   └── diff-gaussian-rasterization/  # diff-gaussian-rasterization子模块
│   └── third_party/                # 第三方依赖
│       ├── Wav2Lip/                # Wav2Lip相关代码
│       └── syncnet_python/         # SyncNet相关代码
│
├── CosyVoice/                      # CosyVoice语音克隆模型
│   ├── README.md                   # CosyVoice使用说明
│   ├── run_cosyvoice.sh            # 语音合成脚本封装
│   ├── test_cosyvoice.py           # 测试脚本
│   ├── pretrained_models/          # 预训练模型目录
│   │   └── CosyVoice2-0.5B/        # CosyVoice2-0.5B模型
│   ├── asset/                      # 资源文件
│   │   ├── zero_shot_prompt.wav    # 默认零样本提示音频
│   │   ├── cross_lingual_prompt.wav # 跨语言提示音频
│   │   ├── vosk-model-small-cn-0.22/  # 中文Vosk模型
│   │   └── vosk-model-small-en-us-0.15/  # 英文Vosk模型
│   └── test_result/                # 测试结果目录
│
├── SyncTalk/                       # SyncTalk模型（备选方案）
│   ├── run_synctalk.sh             # SyncTalk运行脚本
│   ├── download_pretrained.sh      # 下载预训练模型脚本
│   ├── Dockerfile                  # Docker配置
│   └── SyncTalk_Docker调用说明.md  # Docker调用说明
│
└── 文档/                           # 项目文档（部分文档在根目录）
    ├── API配置说明.md              # API配置详细说明
    ├── 项目运行环境说明.md          # 环境配置说明
    ├── 接口改造总结文档.md          # 接口改造历史记录
    ├── 参数配置与前端交互设计说明.md
    ├── 语音克隆参考音频选择功能说明.md
    └── 前端需要修改部分说明文档.md
```

## 四、核心模块说明

### 4.1 Flask应用（app.py）

**主要路由**：
- `GET /`：首页，显示三个功能入口
- `GET/POST /video_generation`：视频生成界面和处理
- `GET/POST /model_training`：模型训练界面和处理
- `GET/POST /chat_system`：实时对话系统界面和处理
- `POST /save_audio`：保存用户录音
- `POST /upload_voice_clone`：上传自定义语音克隆参考音频

**端口配置**：默认5001端口，允许外部访问（host='0.0.0.0'）

### 4.2 后端模块（backend/）

#### 4.2.1 video_generator.py
**功能**：视频生成模块，负责调用底层模型生成说话人头像视频

**主要函数**：
- `generate_video(data)`: 根据参数生成视频
  - 支持TalkingGaussian和SyncTalk两种模型
  - 自动处理路径格式（支持相对路径和绝对路径）
  - 支持GPU选择
  - 支持渲染细节等级（sh_degree）参数

**关键参数**：
- `model_name`: "TalkingGaussian" 或 "SyncTalk"
- `model_param`: 模型路径（如 "output/talking_May"）
- `ref_audio`: 参考音频路径
- `gpu_choice`: GPU选择（"GPU0", "GPU1"等）
- `inference_params`: 推理参数（如sh_degree渲染细节等级）

#### 4.2.2 model_trainer.py
**功能**：模型训练模块，负责训练TalkingGaussian模型

**主要函数**：
- `train_model(data)`: 执行模型训练
  - 自动检查数据是否已预处理
  - 如果未预处理，自动执行预处理
  - 调用三阶段训练脚本（train_mouth → train_face → train_fuse）

**关键参数**：
- `model_choice`: "TalkingGaussian" 或 "SyncTalk"
- `ref_video`: 训练视频路径
- `gpu_choice`: GPU选择
- `epoch`: 训练轮数

#### 4.2.3 chat_engine.py
**功能**：对话系统核心引擎，实现完整的语音交互流程

**主要流程**：
1. **ASR（语音识别）**：`input.wav` → `input.txt`
   - 使用Vosk离线语音识别
   - 支持WebM/Opus等多种音频格式
   - 自动转换为16kHz单声道WAV格式
   - 支持中英文模型切换

2. **LLM（大语言模型）**：`input.txt` → `output.txt`
   - 调用统一的LLM服务接口
   - 支持多平台API（Zhipu、OpenAI、DeepSeek）
   - 自动错误处理和API切换

3. **TTS（语音合成）**：`output.txt` + `prompt_wav` → `tts_output.wav`
   - 使用CosyVoice进行语音克隆
   - 支持预设音色、当前录音、自定义音频三种参考音频选择
   - 支持语言类型选择（中文/英文）
   - 支持语速调节（0.5-2.0倍速）

4. **Video Generation（视频生成）**：`tts_output.wav` → `chat_response.mp4`
   - 调用video_generator模块
   - 使用TalkingGaussian生成说话人头像视频

**主要函数**：
- `chat_response(data)`: 完整的对话流程
- `audio_to_text(input_audio, input_text)`: ASR语音识别
- `text_to_speech_cosyvoice(...)`: CosyVoice语音合成
- `get_voice_clone_reference(...)`: 获取语音克隆参考音频路径

#### 4.2.4 llm_service.py
**功能**：统一的大语言模型服务接口

**特性**：
- 支持多平台API（OpenAI、Zhipu、DeepSeek）
- 配置文件管理（`backend/config/api_config.json`）
- 环境变量支持（优先级高于配置文件）
- 自动API切换（当某个API不可用时）
- API密钥验证

**配置优先级**：环境变量 > 配置文件 > 默认值

### 4.3 TalkingGaussian模块

**技术原理**：基于Gaussian Splatting的3D说话人头像合成

**训练流程**：
1. **数据预处理**：`data_utils/process.py`
   - 提取视频帧
   - 面部跟踪和3DMM拟合
   - 生成transforms_train.json

2. **三阶段训练**：
   - Stage 1：`train_mouth.py` - 嘴部训练
   - Stage 2：`train_face.py` - 面部训练
   - Stage 3：`train_fuse.py` - 融合训练

**推理流程**：
- 音频特征提取（DeepSpeech或HuBERT）
- 使用训练好的模型合成视频
- 视频后处理（裁剪、合成音轨）

**音频特征提取器**：
- DeepSpeech：适用于英文音频
- HuBERT：适用于非英文音频，推荐使用

### 4.4 CosyVoice模块

**技术原理**：零样本语音克隆技术，可以克隆任意说话人的声音

**使用方法**：
- 提供参考音频（prompt_wav）
- 提供参考文本（prompt_text，可选）
- 提供要合成的文本（tts_text）
- 选择语言类型（zh/en）
- 调节语速（0.5-2.0）

**参考音频选择方式**：
1. **当前录音**：使用用户刚录制的音频
2. **预设音色**：使用系统预设的音频（default、cross_lingual等）
3. **自定义音频**：上传自定义的参考音频文件

### 4.5 前端界面

**主要页面**：
1. **首页（index.html）**：
   - 三个功能入口按钮
   - 主题切换功能
   - 全屏模式支持

2. **模型训练页面（model_training.html）**：
   - 模型选择（TalkingGaussian/SyncTalk）
   - 训练视频上传
   - GPU选择
   - 训练参数配置

3. **视频生成页面（video_generation.html）**：
   - 模型选择
   - 模型路径输入
   - 参考音频选择
   - GPU选择
   - 渲染细节等级选择

4. **实时对话页面（chat_system.html）**：
   - 录音功能
   - 模型配置
   - 语音克隆参考音频选择（三种方式）
   - LLM API选择
   - 语言类型选择（中文/英文）
   - 语速调节
   - 视频播放区域

## 五、系统工作流程

### 5.1 模型训练流程

```
用户上传训练视频
    ↓
前端POST请求 → app.py /model_training
    ↓
backend/model_trainer.py → train_model()
    ↓
检查数据预处理状态
    ↓
[未预处理] → 执行 data_utils/process.py 预处理
    ↓
调用 scripts/train_xx.sh 执行三阶段训练
    ↓
返回模型路径（如 "output/talking_May"）
```

### 5.2 视频生成流程

```
用户选择模型和音频
    ↓
前端POST请求 → app.py /video_generation
    ↓
backend/video_generator.py → generate_video()
    ↓
验证模型路径和参数
    ↓
调用 TalkingGaussian/run_talkinggaussian.sh
    ↓
提取音频特征 → 模型推理 → 视频合成
    ↓
返回视频路径
    ↓
前端播放视频
```

### 5.3 实时对话流程

```
用户录音
    ↓
前端POST → /save_audio → 保存到 static/audios/input.wav
    ↓
用户点击"生成视频"
    ↓
前端POST请求 → app.py /chat_system
    ↓
backend/chat_engine.py → chat_response()
    ↓
[步骤1] ASR语音识别
    input.wav → Vosk → input.txt
    ↓
[步骤2] LLM生成回复
    input.txt → llm_service.py → output.txt
    ↓
[步骤3] CosyVoice语音合成
    output.txt + prompt_wav → CosyVoice → tts_output.wav
    ↓
[步骤4] TalkingGaussian视频生成
    tts_output.wav → video_generator.py → chat_response.mp4
    ↓
返回视频路径 → 前端播放视频
```

## 六、配置文件说明

### 6.1 API配置（backend/config/api_config.json）

**配置项**：
```json
{
  "openai": {
    "api_key": "sk-xxxxxxxx",
    "base_url": "https://api.openai.com/v1",
    "model": "gpt-3.5-turbo",
    "enabled": true
  },
  "zhipu": {
    "api_key": "your-zhipu-api-key",
    "base_url": "https://open.bigmodel.cn/api/paas/v4/",
    "model": "glm-4",
    "enabled": true
  },
  "deepseek": {
    "api_key": "sk-your-deepseek-api-key",
    "base_url": "https://api.deepseek.com",
    "model": "deepseek-chat",
    "enabled": true
  }
}
```

**环境变量支持**：
- `OPENAI_API_KEY`
- `ZHIPU_API_KEY`
- `DEEPSEEK_API_KEY`

### 6.2 Python依赖（requirements.txt）

主要依赖：
- Flask==3.0.3
- SpeechRecognition>=3.14.4
- vosk>=0.3.45
- openai>=1.0.0
- zhipuai

## 七、数据目录说明

### 7.1 输入数据
- `static/uploads/videos/<ID>.mp4`：训练视频**推荐入口**（例如 `static/uploads/videos/May.mp4`）
- `TalkingGaussian/data/<ID>/<ID>.mp4`：训练视频在 TalkingGaussian 内部的实际存储路径（由系统从 `static/uploads/videos/` 自动复制）
- `static/audios/input.wav`：用户录音（实时对话）
- `static/audios/custom_voice/*.wav`：自定义语音克隆参考音频

### 7.2 输出数据
- `TalkingGaussian/output/<project_name>/`：训练好的模型（如 `TalkingGaussian/output/May/`）
- `static/uploads/audios/<ID>_reference.wav`：从训练视频自动提取并复制出的参考音频（如 `static/uploads/audios/may_reference.wav`）
- `static/videos/*.mp4`：生成的视频文件（包括视频生成页面与实时对话生成的视频）
- `static/text/input.txt`：ASR识别结果（主文件，会被覆盖，同时保存带时间戳副本）
- `static/text/output.txt`：LLM生成的回复（主文件，会被覆盖，同时保存带时间戳副本）
- `static/audios/tts_output.wav`：语音合成输出（主文件，会被覆盖，同时保存带时间戳副本）

### 7.3 模型文件
- `CosyVoice/pretrained_models/CosyVoice2-0.5B/`：CosyVoice预训练模型
- `CosyVoice/asset/vosk-model-small-cn-0.22/`：中文Vosk模型
- `CosyVoice/asset/vosk-model-small-en-us-0.15/`：英文Vosk模型

## 八、运行说明

### 8.1 Docker方式（推荐）

**详细配置文档**：请参考 `项目配置文档.md`

**快速启动**：
```bash
# 1. 构建镜像
docker-compose build

# 2. 启动服务
docker-compose up -d

# 3. 查看日志
docker-compose logs -f tfg_ui
```

**默认访问地址**：http://服务器IP:5001

### 8.2 本地环境方式（可选）

```bash
# 安装依赖
pip install -r requirements.txt

# 启动Flask应用
python app.py
```

**默认访问地址**：http://127.0.0.1:5001

### 8.3 环境要求

#### Docker方式
1. **Docker**：20.10+
2. **Docker Compose**：1.29+（可选）
3. **NVIDIA Docker**：支持GPU（`nvidia-docker2`）
4. **GPU**：支持CUDA 11.8+（推荐NVIDIA GPU，至少8GB显存）

#### 本地环境方式
1. **Python环境**：Python 3.x
2. **Conda环境**：
   - `talking_gaussian`：用于TalkingGaussian模型
   - `cosyvoice`：用于CosyVoice模型
   - `tg_eval`：用于评测（FID等）
   - `tg_niqe`：用于NIQE评测
3. **GPU支持**：需要CUDA支持的GPU（推荐）
4. **其他工具**：ffmpeg（用于音频格式转换）

## 九、关键特性

### 9.1 路径处理
- 自动路径格式统一（支持Windows/Linux路径差异）
- 相对路径和绝对路径自动识别
- 路径验证和错误处理

### 9.2 参数验证
- 渲染细节等级（sh_degree）：0-3，默认2
- 语速（speed）：0.5-2.0，默认1.0
- 语言类型（language）：zh/en
- GPU选择验证

### 9.3 错误处理
- 模型路径验证
- API调用失败自动切换
- 文件存在性检查
- 详细的错误日志输出

### 9.4 兼容性
- 向后兼容旧的参数格式
- 支持多种音频格式（通过ffmpeg转换）
- 跨平台路径处理

## 十、扩展性

项目采用模块化设计，易于扩展：

1. **新增AI模型**：在`video_generator.py`中添加新的模型分支
2. **新增LLM API**：在`llm_service.py`中添加新的API支持
3. **新增前端功能**：在`templates/`中添加新的HTML页面，在`app.py`中添加对应路由
4. **自定义参数**：通过配置文件或前端表单添加新的参数支持



